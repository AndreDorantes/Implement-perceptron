{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning\n",
        "\n",
        "# U2 - Implementación Perceptrón\n",
        "\n",
        "# Oscar Andre Dorantes Victor\n",
        "\n",
        "# IRC 9B\n",
        "\n",
        "# 10/9/2023"
      ],
      "metadata": {
        "id": "sn7SAXNkLX3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación de la intuición detrás del algoritmo de Perceptrón**"
      ],
      "metadata": {
        "id": "SSn3SJ_ZLgt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perceptron is like a decision making box that takes multiple binary inputs, processes them, and produces a single binary output. It's similar to a voting system where each feature or input has a vote, and the perceptron decides the outcome (1 or 0) based on those votes.\n",
        "\n",
        "A perceptron is like a decision-making committee. Each member votes based on specific factors, but not all votes are equal, some members have more influence, their votes weigh more. The committee's final decision is made when the weighted votes surpass a threshold. If a wrong decision is made, the influence of each member is adjusted to improve future decisions. Committee members are input features, their influence levels are weights, and the threshold is similar to the activation function in a perceptron."
      ],
      "metadata": {
        "id": "-EEA-OcYSm_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudocódigo del algoritmo**"
      ],
      "metadata": {
        "id": "CdBc5f5sLlby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize the training data and labels\n",
        "\n",
        "  X_train <- [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "  y_train <- [0, 0, 0, 1]\n",
        "\n",
        "\n",
        "2. Initialize the perceptron weights and learning rate\n",
        "\n",
        "  weights <- [0, 0, 0]\n",
        "\n",
        "  learning_rate <- 0.1\n",
        "  \n",
        "  epochs <- 100\n",
        "\n",
        "\n",
        "3. Train the perceptron with the training data\n",
        "\n",
        "  FOR epoch IN RANGE(0, epochs) DO\n",
        "\n",
        "      FOR i IN RANGE(0, LENGTH(y_train)) DO\n",
        "\n",
        "      inputs_with_bias <- INSERT 1 AT BEGINNING OF X_train[i]\n",
        "\n",
        "      weighted_sum <- DOT PRODUCT OF weights AND inputs_with_bias\n",
        "            \n",
        "      IF weighted_sum >= 0 THEN\n",
        "\n",
        "      prediction <- 1\n",
        "\n",
        "      ELSE\n",
        "\n",
        "      prediction <- 0\n",
        "            \n",
        "      error <- y_train[i] - prediction\n",
        "              \n",
        "      FOR j IN RANGE(0, LENGTH(weights)) DO\n",
        "\n",
        "      weights[j] <- weights[j] + learning_rate * error *\n",
        "\n",
        "      inputs_with_bias[j]\n",
        "\n",
        "\n",
        "4. Initialize the new data point to be classified\n",
        "\n",
        "  X_new <- [1, 1]\n",
        "\n",
        "\n",
        "5. Classify the new data point using the trained perceptron\n",
        "\n",
        "    inputs_with_bias <- INSERT 1 AT BEGINNING OF X_new\n",
        "    weighted_sum <- DOT PRODUCT OF weights AND inputs_with_bias\n",
        "    \n",
        "    IF weighted_sum >= 0 THEN\n",
        "\n",
        "    prediction <- 1\n",
        "\n",
        "    ELSE\n",
        "\n",
        "    prediction <- 0\n",
        "   \n",
        "\n",
        "6. Print the results\n",
        "\n",
        "    PRINT \"Input:\", X_new\n",
        "\n",
        "    PRINT \"Weights after training:\", weights\n",
        "\n",
        "    PRINT \"Prediction for new input:\", prediction"
      ],
      "metadata": {
        "id": "9g5XHnCAYUR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementación del algoritmo (propia) en Python**"
      ],
      "metadata": {
        "id": "J6Ff4LDQLluE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Training data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  #Inputs\n",
        "y = np.array([0, 0, 0, 1])  #Expected outputs\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, lr=0.1, epochs=100):\n",
        "        self.weights = np.zeros(input_size+1)  #+1 for bias weight\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = self.weights.T.dot(x)\n",
        "        return self.activation(z)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(y.shape[0]):\n",
        "                x = np.insert(X[i], 0, 1)  #Insert 1 for bias weight\n",
        "                predicted = self.predict(x)\n",
        "                error = y[i] - predicted\n",
        "                self.weights = self.weights + self.lr * error * x\n",
        "\n",
        "#Perceptron creation\n",
        "perceptron = Perceptron(input_size=2)\n",
        "\n",
        "#Perceptron training\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "#Perceptron training\n",
        "for i in range(y.shape[0]):\n",
        "    x = np.insert(X[i], 0, 1)  #Insert 1 for bias weight\n",
        "    print(f\"Input: {X[i]}, Prediction: {perceptron.predict(x)}, Actual: {y[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R0tH0nMWW4a",
        "outputId": "d7719775-c7ce-42f1-fe22-55751d01d443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Prediction: 0, Actual: 0\n",
            "Input: [0 1], Prediction: 0, Actual: 0\n",
            "Input: [1 0], Prediction: 0, Actual: 0\n",
            "Input: [1 1], Prediction: 1, Actual: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function + Optimization function identification**"
      ],
      "metadata": {
        "id": "vef8aCnOLmJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function**\n",
        "\n",
        "The perceptron uses the \"hinge loss\" for training. The loss for an individual data point (xi\n",
        "​\n",
        " ,yi\n",
        "​\n",
        " )\n",
        "\n",
        "L(xi\n",
        "​\n",
        " ,yi\n",
        "​\n",
        " ,w)=max(0,−y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b))\n",
        "\n",
        "This means that if a data point is correctly classified and is on the correct side of the decision boundary, its loss is zero. But if it's misclassified or is within the decision boundary, the loss grows linearly the further it is from the boundary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Optimization Function**\n",
        "\n",
        "The goal is to minimize the loss function. The perceptron learning algorithm updates the weights based on misclassified points:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Initialize the weights\n",
        "  **w** and bias\n",
        "  **b** to zero or small random values.\n",
        "*   For each training example,\n",
        "xi\n",
        "​\n",
        " ,yi:\n",
        "\n",
        " If yi(w⋅xi+b) ≤ 0\n",
        "\n",
        "    Update the weights and bias:\n",
        "\n",
        "    w=w+ηyi xi\n",
        "\n",
        "    b=b+ηyi\n",
        "\n",
        "Where\n",
        "η is the learning rate.\n",
        "\n",
        "This update rule pushes the decision boundary to correct the mistake. The algorithm terminates when all points are correctly classified, or a stopping criterion is met (like a set number of iterations).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LKs-9xxpTu81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**"
      ],
      "metadata": {
        "id": "C1LwJmO0SGdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] S. Sharma, “What the Hell is Perceptron? - Towards Data Science,” Medium, Oct. 11, 2019. [Online]. Available: https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53\n",
        "\n",
        "[2] Alexandre, “Perceptron: Concept, function, and applications,” Data Science Courses | DataScientest, Jan. 18, 2023. https://datascientest.com/en/perceptron-definition-and-use-cases\n",
        "\n",
        "[3] M. Banoula, “What is Perceptron: A Beginners Guide for Perceptron,” Simplilearn.com, May 2023, [Online]. Available: https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron"
      ],
      "metadata": {
        "id": "niJswpxoSJLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html \"U2_Implementación_Perceptrón_AndreDorantes.ipynb"
      ],
      "metadata": {
        "id": "LiOMUYIBXh3D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}